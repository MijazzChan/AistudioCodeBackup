{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data14926  data7990\r\n"
     ]
    }
   ],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. This directory will be recovered automatically after resetting environment. \n",
    "!ls /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.\n",
    "# View personal work directory. All changes under this directory will be kept even after reset. Please clean unnecessary files in time to speed up environment loading.\n",
    "!ls /home/aistudio/work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 本地拟合结果\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/1a9b0da9a5724de2adc8c95812f37921cf078702fe3c4cc6b85fc5425b06b315)\n",
    "\n",
    "\n",
    "### 增加的Class\n",
    "```python\n",
    "# Class 1.1 均匀分布(-0.1， 0.1)\n",
    "class SimpleLSTMRNNwithUniformDis(fluid.Layer):\n",
    "\n",
    "# Class 1.2\n",
    "class SentimentClassifier(fluid.Layer):\n",
    "\n",
    "# Class 2.1 高斯分布（0， 0.1）\n",
    "class SimpleLSTMRNNwithNormalDis(fluid.Layer):\n",
    "\n",
    "# Class 2.2\n",
    "class SentimentClassifierwithNormalDis(fluid.Layer):\n",
    "```\n",
    "\n",
    "### 参数修改\n",
    "```python\n",
    "for i in range(self._num_layers):\n",
    "            weight_1 = self.create_parameter(\n",
    "                attr=fluid.ParamAttr(\n",
    "                    initializer=fluid.initializer.NormalInitializer(\n",
    "                        loc=self._init_scale-0.1, scale=self._init_scale)),\n",
    "                shape=[self._hidden_size * 2, self._hidden_size * 4],\n",
    "                dtype=\"float32\",\n",
    "                default_initializer=fluid.initializer.NormalInitializer(\n",
    "                    loc=self._init_scale-0.1, scale=self._init_scale))\n",
    "            self.weight_1_arr.append(self.add_parameter('w_%d' % i, weight_1))\n",
    "            bias_1 = self.create_parameter(\n",
    "                attr=fluid.ParamAttr(\n",
    "                    initializer=fluid.initializer.NormalInitializer(\n",
    "                        loc=self._init_scale-0.1, scale=self._init_scale)),\n",
    "                shape=[self._hidden_size * 4],\n",
    "                dtype=\"float32\",\n",
    "                default_initializer=fluid.initializer.Constant(0.0))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-17 20:24:22,002-INFO: font search path ['/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/ttf', '/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/afm', '/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/mpl-data/fonts/pdfcorefonts']\n",
      "2020-04-17 20:24:22,427-INFO: generated new fontManager\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6867278814315796, 0.7545217275619507, 0.7338612675666809, 0.6918395757675171, 0.6909356117248535, 0.6853599548339844, 0.685122013092041, 0.7102661728858948, 0.7006218433380127, 0.6866329312324524, 0.6823866367340088, 0.6906304955482483, 0.7014378309249878, 0.7034398317337036, 0.6962769031524658, 0.6814473867416382, 0.6836793422698975, 0.711292028427124, 0.6875009536743164, 0.6873851418495178, 0.6974878311157227, 0.6910030841827393, 0.6784456372261047, 0.6959080100059509, 0.7123576402664185, 0.7142702341079712, 0.6968269348144531, 0.6937858462333679, 0.693986177444458, 0.6877716779708862, 0.6924951076507568, 0.6930488348007202, 0.7052773237228394, 0.6904256343841553, 0.6924194097518921, 0.6877864599227905, 0.6976935267448425, 0.6910561323165894, 0.6886035203933716, 0.6934421062469482, 0.6858420372009277, 0.6819095611572266, 0.7189631462097168, 0.6987553238868713, 0.6920435428619385, 0.6971458792686462, 0.6967772245407104, 0.7002228498458862, 0.6900609731674194, 0.6935511827468872, 0.6936644911766052, 0.7005642652511597, 0.6910607218742371, 0.6923110485076904, 0.6936401724815369, 0.7017768621444702, 0.6974459886550903, 0.6946579217910767, 0.6974453926086426, 0.6925167441368103, 0.6839666962623596, 0.6896930932998657, 0.7014224529266357, 0.697127103805542, 0.6814517974853516, 0.7114185094833374, 0.6751473546028137, 0.6726260185241699, 0.8399471044540405, 0.6757974624633789, 0.6922594308853149, 0.7012248039245605, 0.6919098496437073, 0.6820085048675537, 0.6313247084617615, 0.7452535629272461, 0.767391562461853, 0.6824129819869995, 0.6843211650848389, 0.7029050588607788, 0.7013229727745056, 0.7299666404724121, 0.6981806755065918, 0.6707718968391418, 0.6831474900245667, 0.7053331136703491, 0.7136527895927429, 0.6959792375564575, 0.6767964959144592, 0.676620364189148, 0.700791597366333, 0.6938968896865845, 0.6681202054023743, 0.6961686611175537, 0.7209559082984924, 0.6833397150039673, 0.669646143913269, 0.6899358034133911, 0.7115910053253174, 0.6865412592887878, 0.6808621883392334, 0.7074363827705383, 0.6939951181411743, 0.7002602815628052, 0.7027872800827026, 0.6962196826934814, 0.689002275466919, 0.6842759847640991, 0.6754519939422607, 0.7003118991851807, 0.7104552388191223, 0.6822220087051392, 0.693139910697937, 0.6976091861724854, 0.6814842224121094, 0.6657402515411377, 0.6826269030570984, 0.7191813588142395, 0.7019023299217224, 0.685992956161499, 0.6684234142303467, 0.6929080486297607, 0.6797113418579102, 0.695388674736023, 0.6696746945381165, 0.6812326908111572, 0.7121518850326538, 0.6872305870056152, 0.6735271215438843, 0.6594063639640808, 0.732832133769989, 0.6864231824874878, 0.6932855248451233, 0.6817171573638916, 0.7117360830307007, 0.7053453922271729, 0.7138135433197021, 0.6788732409477234, 0.6687904596328735, 0.7070484757423401, 0.69011390209198, 0.6886960864067078, 0.6882352828979492, 0.6856033802032471, 0.6993427276611328, 0.6916494965553284, 0.6712747812271118, 0.6774150133132935, 0.6836411952972412, 0.6819091439247131, 0.6839830875396729, 0.6939012408256531, 0.6883144378662109, 0.6782476305961609, 0.6642698049545288, 0.6936967372894287, 0.6882016658782959, 0.7068288326263428, 0.686486005783081, 0.7063876390457153, 0.6930900812149048, 0.7293394804000854, 0.6658663749694824, 0.7108602523803711, 0.6808179616928101, 0.6509208679199219, 0.6242722272872925, 0.6707327365875244, 0.6382350921630859, 0.675241231918335, 0.6388630867004395, 0.6381392478942871, 0.6639708280563354, 0.6418895721435547, 0.6651021242141724, 0.6337699890136719, 0.6244022846221924, 0.6669409275054932, 0.646766722202301, 0.5973424315452576, 0.6546689867973328, 0.5904381275177002, 0.683203935623169, 0.745547890663147, 0.649737536907196, 0.5653783082962036, 0.7821912169456482, 0.9498888254165649, 0.6246541738510132, 0.617215633392334, 0.6546823382377625, 0.7355961799621582, 0.730100154876709, 0.7045383453369141, 0.6936441659927368, 0.6903083920478821, 0.7172658443450928, 0.7277170419692993, 0.7080490589141846, 0.674427330493927, 0.6881372928619385, 0.6614001393318176, 0.663865327835083, 0.6614459753036499, 0.6888006925582886, 0.6630027890205383, 0.6337087154388428, 0.6570004224777222, 0.6220493316650391, 0.621465802192688, 0.6481113433837891, 0.6740922927856445, 0.5726219415664673, 0.5779770612716675, 0.6439697742462158, 0.5491299629211426, 0.6046102643013, 0.6330471038818359, 0.6165737509727478, 0.6742908954620361, 0.5154683589935303, 0.8032580018043518, 0.6977695822715759, 0.5902341604232788, 0.8215163946151733, 0.7612227201461792, 0.6688052415847778, 0.6802191138267517, 0.557625949382782, 0.5887154340744019, 0.653884768486023, 0.6314907670021057, 0.6427280902862549, 0.672696053981781, 0.6324710845947266, 0.5837039947509766, 0.5263579487800598, 0.6160199642181396, 0.5625370740890503, 0.5645427703857422, 0.5212593078613281, 0.5903310775756836, 0.5558422803878784, 0.561062216758728, 0.5517994165420532, 0.5183713436126709, 0.4627435803413391, 0.6026456952095032, 0.624137818813324, 0.5886925458908081, 0.531033992767334, 0.5218811631202698, 0.5555367469787598, 0.5919449329376221, 0.5594030022621155, 0.5758013129234314, 0.5229780673980713, 0.7268492579460144, 0.5348962545394897, 0.5855759978294373, 0.4529791474342346, 0.6585253477096558, 0.7610816955566406, 0.5098401308059692, 0.6156445145606995, 0.6206105351448059, 0.5627723336219788, 0.6630396842956543, 0.6795334815979004, 0.6225835084915161, 0.6541475057601929, 0.5313238501548767, 0.6113942265510559, 0.6012374758720398, 0.570931077003479, 0.5116323232650757, 0.5357459783554077, 0.6094865798950195, 0.6461783647537231, 0.6597908735275269, 0.528623104095459, 0.5699664354324341, 0.6157308220863342, 0.477122038602829, 0.5380154848098755, 0.4838973581790924, 0.471657395362854, 0.47447091341018677, 0.5984073281288147, 0.4236331582069397, 0.5347635746002197, 0.5239707827568054, 0.45012086629867554, 0.5610529780387878, 0.480557382106781, 0.5394201278686523, 0.42999112606048584, 0.5709830522537231, 0.49631452560424805, 0.6265933513641357, 0.5504791140556335, 0.4647141396999359, 0.5497864484786987, 0.5720118284225464, 0.435960590839386, 0.6419088244438171, 0.585045576095581, 0.4452774226665497, 0.4332529306411743, 0.5674600601196289, 0.45423099398612976, 0.4601345360279083, 0.5250522494316101, 0.46770179271698, 0.48165643215179443, 0.4385237991809845, 0.5038681626319885, 0.5054715275764465, 0.5891246199607849, 0.6875065565109253, 0.5535664558410645, 0.6543236970901489, 0.4636482298374176, 0.4964601397514343, 0.4456643760204315, 0.6045864820480347, 0.4988226890563965, 0.5105424523353577, 0.3914479613304138, 0.4633334279060364, 0.5226861238479614, 0.5879481434822083, 0.5351181030273438, 0.4893074631690979, 0.487318754196167, 0.5463436245918274, 0.3583449125289917, 0.4939853847026825, 0.6459017992019653, 0.42533278465270996, 0.4459056258201599, 0.5831420421600342, 0.5527598261833191, 0.5328649282455444, 0.42278051376342773, 0.46812236309051514, 0.5590243339538574, 0.5054917931556702, 0.536458432674408, 0.490078330039978, 0.5172147154808044, 0.5200779438018799, 0.4551173150539398, 0.5150437355041504, 0.4972904324531555, 0.5208943486213684, 0.4690111577510834, 0.6066250801086426, 0.40441933274269104, 0.4471868872642517, 0.5346583127975464, 0.5704053640365601, 0.46454116702079773, 0.45633751153945923, 0.44853127002716064, 0.47512802481651306, 0.45244935154914856, 0.5236861109733582, 0.5272878408432007, 0.48068714141845703, 0.4582676291465759, 0.4028865098953247, 0.456170916557312, 0.46601569652557373, 0.495583176612854, 0.5769699811935425, 0.4972718358039856, 0.47902989387512207, 0.5195327997207642, 0.5308064222335815, 0.32426998019218445, 0.4112094044685364, 0.3869239091873169, 0.4589860439300537, 0.5458950996398926, 0.4323023855686188, 0.4466652572154999, 0.6149615049362183, 0.5514870882034302, 0.5607646703720093, 0.39105162024497986, 0.2799866199493408, 0.31689220666885376, 0.2817820608615875, 0.4157928228378296, 0.345786452293396, 0.5317733883857727, 0.2821662425994873, 0.37385106086730957, 0.40928617119789124, 0.289079487323761, 0.2999194264411926, 0.27502384781837463, 0.3155348300933838, 0.3803911805152893, 0.2478138506412506, 0.2808432877063751, 0.2390688955783844, 0.30350053310394287, 0.2263685166835785, 0.2260313630104065, 0.30658626556396484, 0.36206763982772827, 0.2646857798099518, 0.3514196276664734, 0.4566313326358795, 0.29430490732192993, 0.3083064556121826, 0.32195940613746643, 0.39609646797180176, 0.4872390031814575, 0.31799542903900146, 0.31049486994743347, 0.23161953687667847, 0.35561585426330566, 0.250606894493103, 0.1956382393836975, 0.2762068808078766, 0.42064911127090454, 0.4006403088569641, 0.3207739293575287, 0.2197316288948059, 0.3730127811431885, 0.4046875834465027, 0.2696564197540283, 0.2505810260772705, 0.32036006450653076, 0.19624197483062744, 0.19855917990207672, 0.25831151008605957, 0.4313996136188507, 0.28509512543678284, 0.3790094554424286, 0.20274315774440765, 0.2666972279548645, 0.21502143144607544, 0.29987087845802307, 0.3306346535682678, 0.2984693646430969, 0.23405753076076508, 0.28471505641937256, 0.31589215993881226, 0.2897244691848755, 0.20065313577651978, 0.22152303159236908, 0.3257085680961609, 0.2632159888744354, 0.24995873868465424, 0.2366744577884674, 0.34794333577156067, 0.31728553771972656, 0.40215975046157837, 0.3208475708961487, 0.2814015746116638, 0.2795695960521698, 0.2721247375011444, 0.29215365648269653, 0.32565897703170776, 0.24439334869384766, 0.35599491000175476, 0.340961217880249, 0.30388614535331726, 0.3865521550178528, 0.35501259565353394, 0.34313732385635376, 0.3552531599998474, 0.16086892783641815, 0.20765697956085205, 0.32668647170066833, 0.11081109941005707, 0.2955707609653473, 0.34601208567619324, 0.37576359510421753, 0.3179999887943268, 0.35041671991348267, 0.29711073637008667, 0.2993999123573303, 0.31462639570236206, 0.20786932110786438, 0.36621415615081787, 0.22362467646598816, 0.25568848848342896, 0.1764879673719406, 0.3587653636932373, 0.27066177129745483, 0.3431027829647064, 0.24626639485359192, 0.3187127113342285, 0.31139063835144043, 0.27879518270492554, 0.33545982837677, 0.27882999181747437, 0.21723808348178864, 0.3375581204891205, 0.26139575242996216, 0.2774185538291931, 0.25773534178733826, 0.23130163550376892, 0.29944732785224915, 0.31127479672431946, 0.4365966320037842, 0.32963114976882935, 0.26812589168548584, 0.21174772083759308, 0.3366953730583191, 0.2392052710056305, 0.29243820905685425, 0.3546941876411438, 0.4350835978984833, 0.20845478773117065, 0.35066986083984375, 0.3093827962875366, 0.2735593914985657, 0.31878456473350525, 0.22610639035701752, 0.16046340763568878, 0.4027445316314697, 0.3172798752784729, 0.3265879452228546, 0.38141879439353943, 0.2917132079601288, 0.17889833450317383, 0.2626612186431885, 0.22980234026908875, 0.2630632519721985, 0.21364599466323853, 0.1683376133441925, 0.2521456480026245, 0.25432661175727844, 0.2505323886871338, 0.36128470301628113, 0.23125940561294556, 0.3536672592163086, 0.20940136909484863, 0.2758408784866333, 0.19533872604370117, 0.1892080008983612, 0.3354857563972473, 0.3746788203716278, 0.23901869356632233, 0.1886066198348999, 0.44643163681030273, 0.28993985056877136, 0.34455519914627075, 0.38240110874176025, 0.3930981159210205, 0.282611608505249, 0.16641323268413544, 0.21377727389335632, 0.41114258766174316, 0.2903038561344147, 0.324895977973938, 0.24833734333515167, 0.2144661545753479, 0.19164970517158508, 0.2940899729728699, 0.29701322317123413, 0.19199694693088531, 0.33940812945365906, 0.2653626501560211, 0.19170044362545013, 0.23806770145893097, 0.34830546379089355, 0.22751928865909576, 0.21667492389678955, 0.33814364671707153, 0.153840571641922, 0.34353765845298767, 0.23973144590854645, 0.21228259801864624, 0.275210976600647, 0.14972396194934845, 0.32381948828697205, 0.3197346329689026, 0.17431437969207764, 0.39050862193107605, 0.20867019891738892, 0.3790889382362366, 0.32437366247177124, 0.18964484333992004, 0.318572998046875, 0.20013879239559174, 0.26823824644088745, 0.23496028780937195, 0.18204490840435028, 0.22678619623184204, 0.2375120222568512, 0.1987273097038269, 0.3334629237651825, 0.4171323776245117, 0.1989997923374176, 0.2908843159675598, 0.3044520914554596, 0.27059870958328247, 0.30630236864089966, 0.31096842885017395, 0.30505508184432983, 0.4823578894138336, 0.2780120372772217, 0.37704208493232727, 0.30767861008644104, 0.34533825516700745, 0.3003508448600769, 0.20011217892169952, 0.2433849573135376, 0.3275940418243408, 0.32632118463516235, 0.23996052145957947, 0.28575849533081055, 0.23612290620803833, 0.2579387128353119, 0.30260369181632996, 0.2533457577228546, 0.3713815212249756, 0.2715107798576355, 0.22671373188495636, 0.2561904489994049, 0.2551701068878174, 0.2668203115463257, 0.3101297616958618, 0.3265208601951599, 0.29116642475128174, 0.3504810929298401, 0.2302594780921936, 0.19428513944149017, 0.2095099687576294, 0.5993886590003967, 0.4075995683670044, 0.29805439710617065, 0.2629709243774414, 0.3201104700565338, 0.1916794776916504, 0.38616326451301575, 0.3917619287967682, 0.19320541620254517, 0.15713202953338623, 0.30033767223358154, 0.4174589514732361, 0.24008765816688538, 0.34064483642578125, 0.2936798334121704, 0.15106359124183655, 0.25103211402893066, 0.11103828251361847, 0.36635851860046387, 0.3427622318267822, 0.3012350797653198, 0.23775506019592285, 0.2997012734413147, 0.24642762541770935, 0.5229438543319702, 0.34711578488349915, 0.3940396010875702, 0.18927189707756042, 0.26979294419288635, 0.2950911819934845, 0.2806127667427063, 0.3076021671295166, 0.287044882774353, 0.26484960317611694, 0.2328004539012909, 0.2025347352027893, 0.20204725861549377, 0.15791410207748413, 0.2638011574745178, 0.4062623083591461, 0.19982948899269104, 0.2833293080329895, 0.258512407541275, 0.3237774670124054, 0.2002575844526291, 0.30351710319519043, 0.26487427949905396, 0.40267762541770935, 0.25736019015312195, 0.3578760027885437, 0.20403653383255005, 0.27145659923553467, 0.21684934198856354, 0.29909974336624146, 0.23671004176139832, 0.33958733081817627, 0.17721867561340332, 0.29589420557022095, 0.14580762386322021, 0.3171401619911194, 0.2224968671798706, 0.25552186369895935, 0.20203790068626404, 0.27547430992126465, 0.2524919807910919, 0.2712976336479187, 0.16497966647148132, 0.29582804441452026, 0.4307023882865906, 0.20220619440078735, 0.17652732133865356, 0.3104135990142822, 0.2192665934562683, 0.2442793846130371, 0.17742137610912323, 0.2839679718017578, 0.24067889153957367, 0.4226471185684204, 0.3990967869758606, 0.3488745093345642, 0.1855701506137848, 0.35259687900543213, 0.1682303249835968, 0.2482602447271347, 0.2716258764266968, 0.3083580732345581, 0.28077155351638794, 0.35095468163490295, 0.35084637999534607, 0.21400457620620728, 0.2433045357465744, 0.30724409222602844, 0.35216590762138367, 0.3144860863685608, 0.3378356099128723, 0.237666517496109, 0.2946969270706177, 0.3178197145462036, 0.31652921438217163, 0.24636663496494293, 0.46204084157943726, 0.2937290072441101, 0.42030221223831177, 0.2886212468147278, 0.2374212145805359, 0.19892752170562744, 0.32121002674102783, 0.24967074394226074, 0.26222914457321167, 0.19032514095306396, 0.22307388484477997, 0.21672159433364868, 0.3269239664077759, 0.33323824405670166, 0.24170684814453125, 0.2790639102458954, 0.21453741192817688, 0.16221100091934204, 0.277266263961792, 0.26098161935806274, 0.45295435190200806, 0.24660062789916992, 0.354391872882843, 0.2629839777946472, 0.3140367269515991, 0.2595102787017822, 0.36774736642837524, 0.3055504560470581, 0.26690182089805603, 0.3011906147003174, 0.49538275599479675, 0.41082894802093506, 0.2590506076812744, 0.29494908452033997, 0.1396556943655014, 0.24799856543540955, 0.2761150598526001, 0.21867939829826355, 0.2404666692018509, 0.34118837118148804, 0.21375299990177155, 0.12238968163728714, 0.22237351536750793, 0.36552608013153076, 0.28752046823501587, 0.11140074580907822, 0.1278870403766632, 0.09356340765953064, 0.13762333989143372, 0.20269188284873962, 0.06412743031978607, 0.1223970353603363, 0.09462785720825195, 0.0944548174738884, 0.1484699845314026, 0.05508365482091904, 0.09312480688095093, 0.12801530957221985, 0.05758393928408623, 0.11845855414867401, 0.08121558278799057, 0.08946406096220016, 0.042809758335351944, 0.12310845404863358, 0.018452022224664688, 0.030064158141613007, 0.03925388678908348, 0.04933713376522064, 0.032087329775094986, 0.02781766466796398, 0.16814124584197998, 0.031263500452041626, 0.10430988669395447, 0.15546056628227234, 0.035043880343437195, 0.021143298596143723, 0.03886585682630539, 0.03852099925279617, 0.23073944449424744, 0.04543560743331909, 0.16266906261444092, 0.11057846993207932, 0.1024848073720932, 0.04901253804564476, 0.013878997415304184, 0.09457497298717499, 0.22970905900001526, 0.04180449992418289, 0.07932156324386597, 0.09619069844484329, 0.008475089445710182, 0.12863248586654663, 0.028345871716737747, 0.16933898627758026, 0.030215229839086533, 0.08600843697786331, 0.031192772090435028, 0.1238999217748642, 0.06874556839466095, 0.07504593580961227, 0.019471794366836548, 0.08565059304237366, 0.16634002327919006, 0.060603830963373184, 0.06649737060070038, 0.056490086019039154, 0.09050479531288147, 0.08856376260519028, 0.11521001160144806, 0.12091433256864548, 0.11570075899362564, 0.034418944269418716, 0.01363249309360981, 0.034396152943372726, 0.10648120939731598, 0.0273110531270504, 0.2070004940032959, 0.02866039052605629, 0.05889670550823212, 0.0288408063352108, 0.052284739911556244, 0.07695631682872772, 0.2107701599597931, 0.17925706505775452, 0.11597783863544464, 0.04533841460943222, 0.0680653303861618, 0.2664162814617157, 0.016640251502394676, 0.06423115730285645, 0.023766381666064262, 0.13977113366127014, 0.07003345340490341, 0.050194744020700455, 0.04895663261413574, 0.14571169018745422, 0.059504806995391846, 0.043965622782707214, 0.03415627032518387, 0.14769458770751953, 0.06122089922428131, 0.024507123976945877, 0.0753345787525177, 0.0700206384062767, 0.0471545085310936, 0.046737998723983765, 0.05575500801205635, 0.10125602036714554, 0.14220109581947327, 0.05308995023369789, 0.054216962307691574, 0.047822266817092896, 0.05230233445763588, 0.0664428323507309, 0.09961385279893875, 0.06582942605018616, 0.029573149979114532, 0.04208750277757645, 0.027427345514297485, 0.011788335628807545, 0.08681230247020721, 0.037718288600444794, 0.03369022533297539, 0.03965892270207405, 0.07294130325317383, 0.02036208286881447, 0.1397128701210022, 0.025461997836828232, 0.016582809388637543, 0.022344591096043587, 0.021505314856767654, 0.054829396307468414, 0.07512007653713226, 0.10429177433252335, 0.03773591294884682, 0.03596600517630577, 0.12404964119195938, 0.0372062586247921, 0.08061665296554565, 0.08282133936882019, 0.32624268531799316, 0.04401635378599167, 0.06715584546327591, 0.03356509655714035, 0.10052258521318436, 0.032380182296037674, 0.14539960026741028, 0.07221430540084839, 0.12243053317070007, 0.18773983418941498, 0.11429096758365631, 0.09743306040763855, 0.11837038397789001, 0.11391295492649078, 0.12529093027114868, 0.09556806832551956, 0.09064376354217529, 0.13754257559776306, 0.0574076883494854, 0.10426129400730133, 0.123870849609375, 0.03535383194684982, 0.052588433027267456, 0.18550556898117065, 0.055366888642311096, 0.11272638291120529, 0.08332653343677521, 0.1002885028719902, 0.13201142847537994, 0.09857248514890671, 0.05381857976317406, 0.06229224056005478, 0.09458982199430466, 0.0765591412782669, 0.08211369812488556, 0.05927976965904236, 0.07442396879196167, 0.1293114423751831, 0.03256215900182724, 0.07742179930210114, 0.09246504306793213, 0.07430911809206009, 0.11300023645162582, 0.09877325594425201, 0.098543681204319, 0.07143884152173996, 0.019477471709251404, 0.07632159441709518, 0.0460868775844574, 0.036473892629146576, 0.0767502561211586, 0.05134788155555725, 0.1058933287858963, 0.03615396097302437, 0.06338208168745041, 0.0328572615981102, 0.13964271545410156, 0.04907959699630737, 0.05973346531391144, 0.09091635793447495, 0.14261049032211304, 0.08420732617378235, 0.20489411056041718, 0.17428457736968994, 0.12142019718885422, 0.20153336226940155, 0.14981381595134735, 0.13101758062839508, 0.1206851452589035, 0.12822355329990387, 0.09277595579624176, 0.16748970746994019, 0.20740604400634766, 0.18299557268619537, 0.08247514069080353, 0.05367070436477661, 0.13837851583957672, 0.142966166138649, 0.1818360686302185, 0.10113465785980225, 0.058908239006996155, 0.02993600256741047, 0.057623490691185, 0.12830448150634766]\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 学习“百度架构师手把手教深度学习”中的课节5中的视频0304和0310，其中0304只要从43分45秒开始就行。\n",
    "# 下面的程序为IMDB情绪分析的具体代码，请完成\n",
    "# 1）LSTM的参数初始化方式对梯度收敛会造成影响，请将下面代码中LSTM模型的参数初始化方式从[-0.1,0.1]的均匀分布改为（0,0.1）的高斯分布，\n",
    "#    并比较两者损失函数收敛谁更快\n",
    "# 2）下面的程序没有对情感分析训练模型进行测试，请从batch中选择一批数据，打印其中每个样本，以及该样本的情感分析结果\n",
    "# 3）修改程序，尽量使用GPU来完成（近期网上学习的人极具增加，导致GPU免费资源不够，若不能使用GPU环境，可能需要多次尝试）\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import six\n",
    "import requests\n",
    "import string\n",
    "import tarfile\n",
    "import hashlib\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import paddle\n",
    "import paddle.fluid as fluid\n",
    "from paddle.fluid.dygraph.nn import Embedding\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def download():\n",
    "    # 通过python的requests类，下载存储在\n",
    "    # https://dataset.bj.bcebos.com/imdb%2FaclImdb_v1.tar.gz的文件\n",
    "    corpus_url = \"https://dataset.bj.bcebos.com/imdb%2FaclImdb_v1.tar.gz\"\n",
    "    web_request = requests.get(corpus_url)\n",
    "    corpus = web_request.content\n",
    "\n",
    "    # 将下载的文件写在当前目录的aclImdb_v1.tar.gz文件内\n",
    "    with open(\"./aclImdb_v1.tar.gz\", \"wb\") as f:\n",
    "        f.write(corpus)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def load_imdb(is_training):\n",
    "    data_set = []\n",
    "\n",
    "    # aclImdb_v1.tar.gz解压后是一个目录\n",
    "    # 我们可以使用python的rarfile库进行解压\n",
    "    # 训练数据和测试数据已经经过切分，其中训练数据的地址为：\n",
    "    # ./aclImdb/train/pos/ 和 ./aclImdb/train/neg/，分别存储着正向情感的数据和负向情感的数据\n",
    "    # 我们把数据依次读取出来，并放到data_set里\n",
    "    # data_set中每个元素都是一个二元组，（句子，label），其中label=0表示负向情感，label=1表示正向情感\n",
    "\n",
    "    for label in [\"pos\", \"neg\"]:\n",
    "        with tarfile.open(\"./aclImdb_v1.tar.gz\") as tarf:\n",
    "            path_pattern = \"aclImdb/train/\" + label + \"/.*\\.txt$\" if is_training \\\n",
    "                else \"aclImdb/test/\" + label + \"/.*\\.txt$\"\n",
    "            path_pattern = re.compile(path_pattern)\n",
    "            tf = tarf.next()\n",
    "            while tf != None:\n",
    "                if bool(path_pattern.match(tf.name)):\n",
    "                    sentence = tarf.extractfile(tf).read().decode()\n",
    "                    sentence_label = 0 if label == 'neg' else 1\n",
    "                    data_set.append((sentence, sentence_label))\n",
    "                tf = tarf.next()\n",
    "\n",
    "    return data_set\n",
    "\n",
    "\n",
    "def data_preprocess(corpus):\n",
    "    data_set = []\n",
    "    for sentence, sentence_label in corpus:\n",
    "        sentence = sentence.strip().lower()\n",
    "        sentence = sentence.split(\" \")\n",
    "\n",
    "        data_set.append((sentence, sentence_label))\n",
    "\n",
    "    return data_set\n",
    "\n",
    "\n",
    "def build_dict(corpus):\n",
    "    word_freq_dict = dict()\n",
    "    for sentence, _ in corpus:\n",
    "        for word in sentence:\n",
    "            if word not in word_freq_dict:\n",
    "                word_freq_dict[word] = 0\n",
    "            word_freq_dict[word] += 1\n",
    "\n",
    "    word_freq_dict = sorted(word_freq_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    word2id_dict = dict()\n",
    "    word2id_freq = dict()\n",
    "\n",
    "    word2id_dict['[oov]'] = 0\n",
    "    word2id_freq[0] = 1e10\n",
    "\n",
    "    word2id_dict['[pad]'] = 1\n",
    "    word2id_freq[1] = 1e10\n",
    "\n",
    "    for word, freq in word_freq_dict:\n",
    "        word2id_dict[word] = len(word2id_dict)\n",
    "        word2id_freq[word2id_dict[word]] = freq\n",
    "\n",
    "    return word2id_freq, word2id_dict\n",
    "\n",
    "\n",
    "def convert_corpus_to_id(corpus, word2id_dict):\n",
    "    data_set = []\n",
    "    for sentence, sentence_label in corpus:\n",
    "        sentence = [word2id_dict[word] if word in word2id_dict \\\n",
    "                        else word2id_dict['[oov]'] for word in sentence]\n",
    "        data_set.append((sentence, sentence_label))\n",
    "    return data_set\n",
    "\n",
    "\n",
    "def build_batch(word2id_dict, corpus, batch_size, epoch_num, max_seq_len, shuffle=True):\n",
    "    sentence_batch = []\n",
    "    sentence_label_batch = []\n",
    "\n",
    "    for _ in range(epoch_num):\n",
    "        if shuffle:\n",
    "            random.shuffle(corpus)\n",
    "\n",
    "        for sentence, sentence_label in corpus:\n",
    "            sentence_sample = sentence[:min(max_seq_len, len(sentence))]\n",
    "            if len(sentence_sample) < max_seq_len:\n",
    "                for _ in range(max_seq_len - len(sentence_sample)):\n",
    "                    sentence_sample.append(word2id_dict['[pad]'])\n",
    "\n",
    "            sentence_sample = [[word_id] for word_id in sentence_sample]\n",
    "\n",
    "            sentence_batch.append(sentence_sample)\n",
    "            sentence_label_batch.append([sentence_label])\n",
    "\n",
    "            if len(sentence_batch) == batch_size:\n",
    "                yield np.array(sentence_batch).astype(\"int64\"), np.array(sentence_label_batch).astype(\"int64\")\n",
    "                sentence_batch = []\n",
    "                sentence_label_batch = []\n",
    "\n",
    "    if len(sentence_batch) == batch_size:\n",
    "        yield np.array(sentence_batch).astype(\"int64\"), np.array(sentence_label_batch).astype(\"int64\")\n",
    "\n",
    "# Class 1.1 均匀分布(-0.1， 0.1)\n",
    "class SimpleLSTMRNNwithUniformDis(fluid.Layer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 hidden_size,\n",
    "                 num_steps,\n",
    "                 num_layers=1,\n",
    "                 init_scale=0.1,\n",
    "                 dropout=None):\n",
    "\n",
    "        super(SimpleLSTMRNNwithUniformDis, self).__init__()\n",
    "        self._hidden_size = hidden_size\n",
    "        self._num_layers = num_layers\n",
    "        self._init_scale = init_scale\n",
    "        self._dropout = dropout\n",
    "        self._input = None\n",
    "        self._num_steps = num_steps\n",
    "        self.cell_array = []\n",
    "        self.hidden_array = []\n",
    "\n",
    "        self.weight_1_arr = []\n",
    "        self.weight_2_arr = []\n",
    "        self.bias_arr = []\n",
    "        self.mask_array = []\n",
    "\n",
    "        for i in range(self._num_layers):\n",
    "            weight_1 = self.create_parameter(\n",
    "                attr=fluid.ParamAttr(\n",
    "                    initializer=fluid.initializer.UniformInitializer(\n",
    "                        low=-self._init_scale, high=self._init_scale)),\n",
    "                shape=[self._hidden_size * 2, self._hidden_size * 4],\n",
    "                dtype=\"float32\",\n",
    "                default_initializer=fluid.initializer.UniformInitializer(\n",
    "                    low=-self._init_scale, high=self._init_scale))\n",
    "            self.weight_1_arr.append(self.add_parameter('w_%d' % i, weight_1))\n",
    "            bias_1 = self.create_parameter(\n",
    "                attr=fluid.ParamAttr(\n",
    "                    initializer=fluid.initializer.UniformInitializer(\n",
    "                        low=-self._init_scale, high=self._init_scale)),\n",
    "                shape=[self._hidden_size * 4],\n",
    "                dtype=\"float32\",\n",
    "                default_initializer=fluid.initializer.Constant(0.0))\n",
    "            self.bias_arr.append(self.add_parameter('b_%d' % i, bias_1))\n",
    "\n",
    "    def forward(self, input_embedding, init_hidden=None, init_cell=None):\n",
    "        self.cell_array = []\n",
    "        self.hidden_array = []\n",
    "\n",
    "        for i in range(self._num_layers):\n",
    "            pre_hidden = fluid.layers.slice(\n",
    "                init_hidden, axes=[0], starts=[i], ends=[i + 1])\n",
    "            pre_cell = fluid.layers.slice(\n",
    "                init_cell, axes=[0], starts=[i], ends=[i + 1])\n",
    "            pre_hidden = fluid.layers.reshape(\n",
    "                pre_hidden, shape=[-1, self._hidden_size])\n",
    "            pre_cell = fluid.layers.reshape(\n",
    "                pre_cell, shape=[-1, self._hidden_size])\n",
    "            self.hidden_array.append(pre_hidden)\n",
    "            self.cell_array.append(pre_cell)\n",
    "\n",
    "        res = []\n",
    "        for index in range(self._num_steps):\n",
    "            self._input = fluid.layers.slice(\n",
    "                input_embedding, axes=[1], starts=[index], ends=[index + 1])\n",
    "            self._input = fluid.layers.reshape(\n",
    "                self._input, shape=[-1, self._hidden_size])\n",
    "\n",
    "            for k in range(self._num_layers):\n",
    "                pre_hidden = self.hidden_array[k]\n",
    "                pre_cell = self.cell_array[k]\n",
    "                weight_1 = self.weight_1_arr[k]\n",
    "                bias = self.bias_arr[k]\n",
    "\n",
    "                nn = fluid.layers.concat([self._input, pre_hidden], 1)\n",
    "                gate_input = fluid.layers.matmul(x=nn, y=weight_1)\n",
    "                gate_input = fluid.layers.elementwise_add(gate_input, bias)\n",
    "\n",
    "                i, j, f, o = fluid.layers.split(\n",
    "                    gate_input, num_or_sections=4, dim=-1)\n",
    "\n",
    "                c = pre_cell * fluid.layers.sigmoid(f) + fluid.layers.sigmoid(\n",
    "                    i) * fluid.layers.tanh(j)\n",
    "                m = fluid.layers.tanh(c) * fluid.layers.sigmoid(o)\n",
    "\n",
    "                self.hidden_array[k] = m\n",
    "                self.cell_array[k] = c\n",
    "                self._input = m\n",
    "\n",
    "                if self._dropout is not None and self._dropout > 0.0:\n",
    "                    self._input = fluid.layers.dropout(\n",
    "                        self._input,\n",
    "                        dropout_prob=self._dropout,\n",
    "                        dropout_implementation='upscale_in_train')\n",
    "\n",
    "            res.append(\n",
    "                fluid.layers.reshape(\n",
    "                    self._input, shape=[1, -1, self._hidden_size]))\n",
    "\n",
    "        real_res = fluid.layers.concat(res, 0)\n",
    "        real_res = fluid.layers.transpose(x=real_res, perm=[1, 0, 2])\n",
    "        last_hidden = fluid.layers.concat(self.hidden_array, 1)\n",
    "        last_hidden = fluid.layers.reshape(\n",
    "            last_hidden, shape=[-1, self._num_layers, self._hidden_size])\n",
    "        last_hidden = fluid.layers.transpose(x=last_hidden, perm=[1, 0, 2])\n",
    "        last_cell = fluid.layers.concat(self.cell_array, 1)\n",
    "        last_cell = fluid.layers.reshape(\n",
    "            last_cell, shape=[-1, self._num_layers, self._hidden_size])\n",
    "        last_cell = fluid.layers.transpose(x=last_cell, perm=[1, 0, 2])\n",
    "\n",
    "        return real_res, last_hidden, last_cell\n",
    "\n",
    "# Class 1.2\n",
    "class SentimentClassifier(fluid.Layer):\n",
    "    def __init__(self,\n",
    "                 hidden_size,\n",
    "                 vocab_size,\n",
    "                 class_num=2,\n",
    "                 num_layers=1,\n",
    "                 num_steps=128,\n",
    "                 init_scale=0.1,\n",
    "                 dropout=None):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.class_num = class_num\n",
    "        self.init_scale = init_scale\n",
    "        self.num_layers = num_layers\n",
    "        self.num_steps = num_steps\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.simple_lstm_rnn = SimpleLSTMRNNwithUniformDis(\n",
    "            hidden_size,\n",
    "            num_steps,\n",
    "            num_layers=num_layers,\n",
    "            init_scale=init_scale,\n",
    "            dropout=dropout)\n",
    "\n",
    "        self.embedding = Embedding(\n",
    "            size=[vocab_size, hidden_size],\n",
    "            dtype='float32',\n",
    "            is_sparse=False,\n",
    "            param_attr=fluid.ParamAttr(\n",
    "                name='embedding_para',\n",
    "                initializer=fluid.initializer.UniformInitializer(\n",
    "                    low=-init_scale, high=init_scale)))\n",
    "\n",
    "        self.softmax_weight = self.create_parameter(\n",
    "            attr=fluid.ParamAttr(),\n",
    "            shape=[self.hidden_size, self.class_num],\n",
    "            dtype=\"float32\",\n",
    "            default_initializer=fluid.initializer.UniformInitializer(\n",
    "                low=-self.init_scale, high=self.init_scale))\n",
    "        self.softmax_bias = self.create_parameter(\n",
    "            attr=fluid.ParamAttr(),\n",
    "            shape=[self.class_num],\n",
    "            dtype=\"float32\",\n",
    "            default_initializer=fluid.initializer.UniformInitializer(\n",
    "                low=-self.init_scale, high=self.init_scale))\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        init_hidden_data = np.zeros(\n",
    "            (1, batch_size, embedding_size), dtype='float32')\n",
    "        init_cell_data = np.zeros(\n",
    "            (1, batch_size, embedding_size), dtype='float32')\n",
    "\n",
    "        init_hidden = fluid.dygraph.to_variable(init_hidden_data)\n",
    "        init_hidden.stop_gradient = True\n",
    "        init_cell = fluid.dygraph.to_variable(init_cell_data)\n",
    "        init_cell.stop_gradient = True\n",
    "\n",
    "        init_h = fluid.layers.reshape(\n",
    "            init_hidden, shape=[self.num_layers, -1, self.hidden_size])\n",
    "\n",
    "        init_c = fluid.layers.reshape(\n",
    "            init_cell, shape=[self.num_layers, -1, self.hidden_size])\n",
    "\n",
    "        x_emb = self.embedding(input)\n",
    "\n",
    "        x_emb = fluid.layers.reshape(\n",
    "            x_emb, shape=[-1, self.num_steps, self.hidden_size])\n",
    "        if self.dropout is not None and self.dropout > 0.0:\n",
    "            x_emb = fluid.layers.dropout(\n",
    "                x_emb,\n",
    "                dropout_prob=self.dropout,\n",
    "                dropout_implementation='upscale_in_train')\n",
    "\n",
    "        rnn_out, last_hidden, last_cell = self.simple_lstm_rnn(x_emb, init_h,\n",
    "                                                               init_c)\n",
    "        last_hidden = fluid.layers.reshape(\n",
    "            last_hidden, shape=[-1, self.hidden_size])\n",
    "\n",
    "        projection = fluid.layers.matmul(last_hidden, self.softmax_weight)\n",
    "        projection = fluid.layers.elementwise_add(projection, self.softmax_bias)\n",
    "        projection = fluid.layers.reshape(\n",
    "            projection, shape=[-1, self.class_num])\n",
    "        pred = fluid.layers.softmax(projection, axis=-1)\n",
    "\n",
    "        loss = fluid.layers.softmax_with_cross_entropy(\n",
    "            logits=projection, label=label, soft_label=False)\n",
    "        loss = fluid.layers.reduce_mean(loss)\n",
    "\n",
    "        return pred, loss\n",
    "\n",
    "# Class 2.1 高斯分布（0， 0.1）\n",
    "class SimpleLSTMRNNwithNormalDis(fluid.Layer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 hidden_size,\n",
    "                 num_steps,\n",
    "                 num_layers=1,\n",
    "                 init_scale=0.1,\n",
    "                 dropout=None):\n",
    "\n",
    "        super(SimpleLSTMRNNwithNormalDis, self).__init__()\n",
    "        self._hidden_size = hidden_size\n",
    "        self._num_layers = num_layers\n",
    "        self._init_scale = init_scale\n",
    "        self._dropout = dropout\n",
    "        self._input = None\n",
    "        self._num_steps = num_steps\n",
    "        self.cell_array = []\n",
    "        self.hidden_array = []\n",
    "\n",
    "        self.weight_1_arr = []\n",
    "        self.weight_2_arr = []\n",
    "        self.bias_arr = []\n",
    "        self.mask_array = []\n",
    "\n",
    "        for i in range(self._num_layers):\n",
    "            weight_1 = self.create_parameter(\n",
    "                attr=fluid.ParamAttr(\n",
    "                    initializer=fluid.initializer.NormalInitializer(\n",
    "                        loc=self._init_scale-0.1, scale=self._init_scale)),\n",
    "                shape=[self._hidden_size * 2, self._hidden_size * 4],\n",
    "                dtype=\"float32\",\n",
    "                default_initializer=fluid.initializer.NormalInitializer(\n",
    "                    loc=self._init_scale-0.1, scale=self._init_scale))\n",
    "            self.weight_1_arr.append(self.add_parameter('w_%d' % i, weight_1))\n",
    "            bias_1 = self.create_parameter(\n",
    "                attr=fluid.ParamAttr(\n",
    "                    initializer=fluid.initializer.NormalInitializer(\n",
    "                        loc=self._init_scale-0.1, scale=self._init_scale)),\n",
    "                shape=[self._hidden_size * 4],\n",
    "                dtype=\"float32\",\n",
    "                default_initializer=fluid.initializer.Constant(0.0))\n",
    "            self.bias_arr.append(self.add_parameter('b_%d' % i, bias_1))\n",
    "\n",
    "    def forward(self, input_embedding, init_hidden=None, init_cell=None):\n",
    "        self.cell_array = []\n",
    "        self.hidden_array = []\n",
    "\n",
    "        for i in range(self._num_layers):\n",
    "            pre_hidden = fluid.layers.slice(\n",
    "                init_hidden, axes=[0], starts=[i], ends=[i + 1])\n",
    "            pre_cell = fluid.layers.slice(\n",
    "                init_cell, axes=[0], starts=[i], ends=[i + 1])\n",
    "            pre_hidden = fluid.layers.reshape(\n",
    "                pre_hidden, shape=[-1, self._hidden_size])\n",
    "            pre_cell = fluid.layers.reshape(\n",
    "                pre_cell, shape=[-1, self._hidden_size])\n",
    "            self.hidden_array.append(pre_hidden)\n",
    "            self.cell_array.append(pre_cell)\n",
    "\n",
    "        res = []\n",
    "        for index in range(self._num_steps):\n",
    "            self._input = fluid.layers.slice(\n",
    "                input_embedding, axes=[1], starts=[index], ends=[index + 1])\n",
    "            self._input = fluid.layers.reshape(\n",
    "                self._input, shape=[-1, self._hidden_size])\n",
    "\n",
    "            for k in range(self._num_layers):\n",
    "                pre_hidden = self.hidden_array[k]\n",
    "                pre_cell = self.cell_array[k]\n",
    "                weight_1 = self.weight_1_arr[k]\n",
    "                bias = self.bias_arr[k]\n",
    "\n",
    "                nn = fluid.layers.concat([self._input, pre_hidden], 1)\n",
    "                gate_input = fluid.layers.matmul(x=nn, y=weight_1)\n",
    "                gate_input = fluid.layers.elementwise_add(gate_input, bias)\n",
    "\n",
    "                i, j, f, o = fluid.layers.split(\n",
    "                    gate_input, num_or_sections=4, dim=-1)\n",
    "\n",
    "                c = pre_cell * fluid.layers.sigmoid(f) + fluid.layers.sigmoid(\n",
    "                    i) * fluid.layers.tanh(j)\n",
    "                m = fluid.layers.tanh(c) * fluid.layers.sigmoid(o)\n",
    "\n",
    "                self.hidden_array[k] = m\n",
    "                self.cell_array[k] = c\n",
    "                self._input = m\n",
    "\n",
    "                if self._dropout is not None and self._dropout > 0.0:\n",
    "                    self._input = fluid.layers.dropout(\n",
    "                        self._input,\n",
    "                        dropout_prob=self._dropout,\n",
    "                        dropout_implementation='upscale_in_train')\n",
    "\n",
    "            res.append(\n",
    "                fluid.layers.reshape(\n",
    "                    self._input, shape=[1, -1, self._hidden_size]))\n",
    "\n",
    "        real_res = fluid.layers.concat(res, 0)\n",
    "        real_res = fluid.layers.transpose(x=real_res, perm=[1, 0, 2])\n",
    "        last_hidden = fluid.layers.concat(self.hidden_array, 1)\n",
    "        last_hidden = fluid.layers.reshape(\n",
    "            last_hidden, shape=[-1, self._num_layers, self._hidden_size])\n",
    "        last_hidden = fluid.layers.transpose(x=last_hidden, perm=[1, 0, 2])\n",
    "        last_cell = fluid.layers.concat(self.cell_array, 1)\n",
    "        last_cell = fluid.layers.reshape(\n",
    "            last_cell, shape=[-1, self._num_layers, self._hidden_size])\n",
    "        last_cell = fluid.layers.transpose(x=last_cell, perm=[1, 0, 2])\n",
    "\n",
    "        return real_res, last_hidden, last_cell\n",
    "\n",
    "# Class 2.2\n",
    "class SentimentClassifierwithNormalDis(fluid.Layer):\n",
    "    def __init__(self,\n",
    "                 hidden_size,\n",
    "                 vocab_size,\n",
    "                 class_num=2,\n",
    "                 num_layers=1,\n",
    "                 num_steps=128,\n",
    "                 init_scale=0.1,\n",
    "                 dropout=None):\n",
    "        super(SentimentClassifierwithNormalDis, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.class_num = class_num\n",
    "        self.init_scale = init_scale\n",
    "        self.num_layers = num_layers\n",
    "        self.num_steps = num_steps\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.simple_lstm_rnn = SimpleLSTMRNNwithNormalDis(\n",
    "            hidden_size,\n",
    "            num_steps,\n",
    "            num_layers=num_layers,\n",
    "            init_scale=init_scale,\n",
    "            dropout=dropout)\n",
    "\n",
    "        self.embedding = Embedding(\n",
    "            size=[vocab_size, hidden_size],\n",
    "            dtype='float32',\n",
    "            is_sparse=False,\n",
    "            param_attr=fluid.ParamAttr(\n",
    "                name='embedding_para',\n",
    "                initializer=fluid.initializer.UniformInitializer(\n",
    "                    low=-init_scale, high=init_scale)))\n",
    "\n",
    "        self.softmax_weight = self.create_parameter(\n",
    "            attr=fluid.ParamAttr(),\n",
    "            shape=[self.hidden_size, self.class_num],\n",
    "            dtype=\"float32\",\n",
    "            default_initializer=fluid.initializer.UniformInitializer(\n",
    "                low=-self.init_scale, high=self.init_scale))\n",
    "        self.softmax_bias = self.create_parameter(\n",
    "            attr=fluid.ParamAttr(),\n",
    "            shape=[self.class_num],\n",
    "            dtype=\"float32\",\n",
    "            default_initializer=fluid.initializer.UniformInitializer(\n",
    "                low=-self.init_scale, high=self.init_scale))\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        init_hidden_data = np.zeros(\n",
    "            (1, batch_size, embedding_size), dtype='float32')\n",
    "        init_cell_data = np.zeros(\n",
    "            (1, batch_size, embedding_size), dtype='float32')\n",
    "\n",
    "        init_hidden = fluid.dygraph.to_variable(init_hidden_data)\n",
    "        init_hidden.stop_gradient = True\n",
    "        init_cell = fluid.dygraph.to_variable(init_cell_data)\n",
    "        init_cell.stop_gradient = True\n",
    "\n",
    "        init_h = fluid.layers.reshape(\n",
    "            init_hidden, shape=[self.num_layers, -1, self.hidden_size])\n",
    "\n",
    "        init_c = fluid.layers.reshape(\n",
    "            init_cell, shape=[self.num_layers, -1, self.hidden_size])\n",
    "\n",
    "        x_emb = self.embedding(input)\n",
    "\n",
    "        x_emb = fluid.layers.reshape(\n",
    "            x_emb, shape=[-1, self.num_steps, self.hidden_size])\n",
    "        if self.dropout is not None and self.dropout > 0.0:\n",
    "            x_emb = fluid.layers.dropout(\n",
    "                x_emb,\n",
    "                dropout_prob=self.dropout,\n",
    "                dropout_implementation='upscale_in_train')\n",
    "\n",
    "        rnn_out, last_hidden, last_cell = self.simple_lstm_rnn(x_emb, init_h,\n",
    "                                                               init_c)\n",
    "        last_hidden = fluid.layers.reshape(\n",
    "            last_hidden, shape=[-1, self.hidden_size])\n",
    "\n",
    "        projection = fluid.layers.matmul(last_hidden, self.softmax_weight)\n",
    "        projection = fluid.layers.elementwise_add(projection, self.softmax_bias)\n",
    "        projection = fluid.layers.reshape(\n",
    "            projection, shape=[-1, self.class_num])\n",
    "        pred = fluid.layers.softmax(projection, axis=-1)\n",
    "\n",
    "        loss = fluid.layers.softmax_with_cross_entropy(\n",
    "            logits=projection, label=label, soft_label=False)\n",
    "        loss = fluid.layers.reduce_mean(loss)\n",
    "\n",
    "        return pred, loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 下载数据集\n",
    "# download()\n",
    "# 生成训练样本和测试样本集\n",
    "train_corpus = load_imdb(True)\n",
    "test_corpus = load_imdb(False)\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"sentence %d, %s\" % (i, train_corpus[i][0]))\n",
    "    print(\"sentence %d, label %d\" % (i, train_corpus[i][1]))\n",
    "# 预处理\n",
    "train_corpus = data_preprocess(train_corpus)\n",
    "test_corpus = data_preprocess(test_corpus)\n",
    "print(train_corpus[:5])\n",
    "\n",
    "# 计算词频，生成词Id\n",
    "word2id_freq, word2id_dict = build_dict(train_corpus)\n",
    "vocab_size = len(word2id_freq)\n",
    "print(\"there are totoally %d different words in the corpus\" % vocab_size)\n",
    "for _, (word, word_id) in zip(range(50), word2id_dict.items()):\n",
    "    print(\"word %s, its id %d, its word freq %d\" % (word, word_id, word2id_freq[word_id]))\n",
    "\n",
    "# 将句子转换为Id序列\n",
    "train_corpus = convert_corpus_to_id(train_corpus, word2id_dict)\n",
    "test_corpus = convert_corpus_to_id(test_corpus, word2id_dict)\n",
    "print(\"%d tokens in the corpus\" % len(train_corpus))\n",
    "print(train_corpus[:5])\n",
    "print(test_corpus[:5])\n",
    "\n",
    "# 生成mini-batch\n",
    "for _, batch in zip(range(10), build_batch(word2id_dict,\n",
    "                                           train_corpus, batch_size=3, epoch_num=3, max_seq_len=30)):\n",
    "    print(batch)\n",
    "\n",
    "# 开始训练\n",
    "batch_size = 64\n",
    "epoch_num = 5\n",
    "embedding_size = 128\n",
    "step = 0\n",
    "learning_rate = 0.01\n",
    "max_seq_len = 128\n",
    "\n",
    "# GPU Flag here\n",
    "use_gpu = True\n",
    "place = fluid.CUDAPlace(0) if use_gpu else fluid.CPUPlace()\n",
    "with fluid.dygraph.guard(place=place):\n",
    "    # 创建一个用于情感分类的网络实例，sentiment_classifier\n",
    "    sentiment_classifier = SentimentClassifier(\n",
    "        embedding_size, vocab_size, num_steps=max_seq_len)\n",
    "    # 创建优化器AdamOptimizer，用于更新这个网络的参数\n",
    "    adam = fluid.optimizer.AdamOptimizer(learning_rate=learning_rate, parameter_list=sentiment_classifier.parameters())\n",
    "    steps1_x = []\n",
    "    steps1_y = []\n",
    "    for sentences, labels in build_batch(\n",
    "            word2id_dict, train_corpus, batch_size, epoch_num, max_seq_len):\n",
    "\n",
    "        sentences_var = fluid.dygraph.to_variable(sentences)\n",
    "        labels_var = fluid.dygraph.to_variable(labels)\n",
    "        pred, loss = sentiment_classifier(sentences_var, labels_var)\n",
    "\n",
    "        loss.backward()\n",
    "        adam.minimize(loss)\n",
    "        sentiment_classifier.clear_gradients()\n",
    "\n",
    "        step += 1\n",
    "        steps1_x.append(step)\n",
    "        steps1_y.append(float(loss.numpy()[0]))\n",
    "        if step % 10 == 0:\n",
    "            print(\"step %d, loss %.3f\" % (step, loss.numpy()[0]))\n",
    "\n",
    "        if step == 1000:\n",
    "            break\n",
    "\n",
    "\n",
    "with fluid.dygraph.guard(place=place):\n",
    "    sentiment_classifier = SentimentClassifierwithNormalDis(\n",
    "        embedding_size, vocab_size, num_steps=max_seq_len)\n",
    "    # 创建优化器AdamOptimizer，用于更新这个网络的参数\n",
    "    adam = fluid.optimizer.AdamOptimizer(learning_rate=learning_rate, parameter_list=sentiment_classifier.parameters())\n",
    "\n",
    "    steps2_x = []\n",
    "    steps2_y = []\n",
    "    step = 0\n",
    "    for sentences, labels in build_batch(\n",
    "            word2id_dict, train_corpus, batch_size, epoch_num, max_seq_len):\n",
    "\n",
    "        sentences_var = fluid.dygraph.to_variable(sentences)\n",
    "        labels_var = fluid.dygraph.to_variable(labels)\n",
    "        pred, loss = sentiment_classifier(sentences_var, labels_var)\n",
    "\n",
    "        loss.backward()\n",
    "        adam.minimize(loss)\n",
    "        sentiment_classifier.clear_gradients()\n",
    "        steps2_x.append(step)\n",
    "        steps2_y.append(float(loss.numpy()[0]))\n",
    "        step += 1\n",
    "        if step % 10 == 0:\n",
    "            print(\"step %d, loss %.3f\" % (step, loss.numpy()[0]))\n",
    "\n",
    "        if step == 1000:\n",
    "            break\n",
    "        \n",
    "            \n",
    "print(steps1_y)\n",
    "print(steps2_y)\n",
    "\n",
    "fig, left_axis = plt.subplots()\n",
    "\n",
    "p1, = left_axis.plot(steps1_x, steps1_y, 'ro-')\n",
    "right_axis = left_axis.twinx()\n",
    "p2, = right_axis.plot(steps2_x, steps2_y, 'bo-')\n",
    "plt.xticks(steps1_x, steps2_x, rotation=0)  # 设置x轴的显示形式\n",
    "\n",
    "# 设置左坐标轴以及右坐标轴的范围、精度\n",
    "left_axis.set_ylim(0, 1.01)\n",
    "left_axis.set_yticks(np.arange(0, 1.01, 0.1))\n",
    "right_axis.set_ylim(0, 1.01)\n",
    "right_axis.set_yticks(np.arange(0, 1.01, 0.1))\n",
    "\n",
    "# 设置坐标及标题的大小、颜色\n",
    "left_axis.set_title('Loss Rate of different param distribution method')\n",
    "left_axis.set_xlabel('Steps')\n",
    "left_axis.set_ylabel('Uniform Distribution', color='r')\n",
    "left_axis.tick_params(axis='y', colors='r')\n",
    "right_axis.set_ylabel('Normal Distribution', color='b')\n",
    "right_axis.tick_params(axis='y', colors='b')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 1.7.1 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
